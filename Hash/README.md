## 什么是哈希算法？

## 哈希算法

哈希算法（**Hash**）：将任意长度的二进制值串映射为固定长度的二进制值串的规则

## 哈希算法的特征

- 输出值的数据长度不变
  - 不管输入数据有多小，哈希值的数据长度相同
- 相同的输入，相同的输出
- 相似的输入，输出大不相同
  - 即使相差一个 bite
- 完全不同的输入，可能相同的输出
  - 概率非常小
  - 哈希冲突
- 单向输入输出（也叫单向哈希算法）
  - 从哈希值不能反向推导原始数据
- 计算相对简单
  - 效率要高

## 函数函数的原理

to do

## 应用场景

### 安全加密

常用于加密的哈希算法

- MD5（MD5Message-Digest Algorithm，MD5 消息摘要算法）
- SHA（Secure Hash Algorithm，安全散列算法）
- DES（Data Encryption Standard，数据加密标准）
- AES（Advanced Encryption Standard，高级加密标准）

加密算法的额外要求：

- 很难根据哈希值反向推导出原始数据
- 散列冲突的概率要很小

> 理论上是没办法做到完全不冲突的
>
> 鸽巢原理（也叫抽屉原理):
> 但是，哈希值的范围很大，（冲突的概率极低）

- MD5，有 2^128 个不同的哈希值（天文数字）
- 散列冲突的概率要小于 1/2^128（冲突的概率极低）
- 很难破解

### 唯一标识

举个栗子：

在海量的图库中，搜索一张图是否存在

- 思路一：单纯地用图片的元信息（比如图片名称）来比对
  - 可能存在名称相同但图片内容不同的情况
  - 或者名称不同图片内容相同的情况
- 思路二：要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对
  - 可行，（本办法）
  - 但消耗大
- 思路三：标识
  - 给每一个图片取一个唯一标识（信息摘要）
  - 可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，
  - 然后将这 300 个字节放到一块，
  - 通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识
  - 通过这个唯一标识来判定图片是否在图库中，就可以减少很多工作量

### 数据校验

举个栗子：

BT 下载的原理

- 从多个机器上并行下载一个大文件
- 大文件可能会被分割成多文件块
- 所有的文件块下载完成之后，再组装成一个完整的大文件

网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的

如何来校验文件块的安全、正确、完整呢？

- 通过哈希算法，对文件块分别取哈希值，并且保存在种子文件中
- 当文件块下载完成之后，
  - 通过相同的哈希算法，对下载好的文件块逐一求哈希值，
  - 然后跟种子文件中保存的哈希值比对
  - 如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块

### 散列函数

散列函数是设计一个散列表的关键（决定了散列冲突的概率和散列表的性能）

- 散列函数对于散列算法冲突的要求不是很高
  - 可以通过开放寻址法或者链表法解决
- 不关心反向解密
- 关注散列后的值是否能平均分布
- 简单，效率高

分布式系统相关应用场景

### 负载均衡

会话粘滞（session sticky）的负载均衡算法，

- 需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上

思路一：维护一张映射关系表：客户端 IP 地址或者会话 ID 与服务器编号的映射关系

- 客户端发出的每次请求，
  - 都要先在映射表中查找应该路由到的服务器编号，
  - 然后再请求编号对应的服务器
- 缺点：
  - 如果客户端很多，映射表可能会很大，比较浪费内存空间；
  - 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大

思路二：借助哈希算法

- 对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行**取模**运算，
- 最终得到的值就是应该被路由到的服务器编号

### 数据分片

举个栗子：统计“搜索关键词”出现的次数

如何快速统计出 1T 的日志文件里每个关键词被搜索的次数？

- 难点一：搜索日志很大
- 难点二：单台机器处理巨大文件耗时太长
- 解决思路：数据分片，多台机器处理
  - 用 n 台机器并行处理
  - 从搜索记录的日志文件中，依次读出每个搜索关键词，
  - 并且通过哈希函数计算哈希值，
  - 然后再跟 n **取模**，最终得到的值，就是应该被分配到的机器编号
  - 哈希值相同的搜索关键词就被分配到了同一个机器上
  - 每台机器会分别计算关键词出现的次数，最后合并起来就是最终的结果
- MapReduce 的基本设计思想

举个栗子：

如何快速判断图片是否在图库中？

- 思路：
  - 给每个图片取唯一标识（或者信息摘要），然后构建散列表
  - 对数据分片，采用多机处理
    - 让每台机器只维护某一部分图片对应的散列表
    - 每次从图库中读取一个图片，计算唯一标识，
    - 然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号
    - 然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表
- 估算给这 1 亿张图片构建散列表大约需要多少台机器
  - 散列表中每个数据单元包含两个信息
    - 哈希值
      - 假设通过 MD5 来计算哈希值
      - 长度就是 128 比特（ 16 字节）
    - 图片文件的路径
      - 文件路径长度的上限是 256 字节
      - 假设平均长度是 128 字节
    - 用链表法来解决冲突
      - 需要存储指针
      - 指针只占用 8 字节
  - 散列表中每个数据单元约占用 152 字节
  - 假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，
    - 一台机器可以给大约 1000 万（2GB\*0.75/152）张图片构建散列表
    - 对 1 亿张图片构建索引，需要大约十几台机器

针对这种海量数据（**大数据**）的处理问题

- 可以采用多机分布式处理

- 借助分片的思路，可以突破单机内存、CPU 等资源的限制

### 分布式存储

场景：分布式缓存（将海量的数据分布在多台机器上）

如何决定将哪个数据放到哪个机器上呢？

- 分片的思想
  - 通过哈希算法对数据取哈希值
  - 然后对机器个数取模，
  - 最终值就是应该存储的缓存机器编号
- 数据增多了呢？
  - 扩容
  - 机器的数量变化了
  - 取模的值变化了
  - 所有的数据都要重新计算哈希值
  - 然后重新搬移到正确的机器上
  - 雪崩效应
- 解决方案：一致性哈希算法
  - 假设有 k 个机器，数据的哈希值的范围是 [0, MAX]
  - 将整个范围划分成 m 个小区间（m 远大于 k）
  - 每个机器负责 m/k 个小区间
  - 当有新机器加入的时候
    - 就将某几个小区间的数据，从原来的机器中搬移到新的机器中
    - 既不用全部重新哈希、搬移数据
    - 也保持了各个机器上数据数量的均衡
